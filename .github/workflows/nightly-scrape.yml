name: Nightly Scrape

on:
  schedule:
    # Run at 3am UTC daily (after PriceCharting updates)
    - cron: '0 3 * * *'
  workflow_dispatch:  # Allow manual trigger

env:
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
  SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_KEY }}

jobs:
  # ── 4 parallel batch jobs ─────────────────────────────────────────────────
  batch1:
    runs-on: ubuntu-latest
    timeout-minutes: 300  # 5 hours max
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: pip install requests beautifulsoup4
      - name: Run batch 1
        run: python pokeprices_scraper_v6.py --sets-file batches/batch1.txt

  batch2:
    runs-on: ubuntu-latest
    timeout-minutes: 300
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: pip install requests beautifulsoup4
      - name: Run batch 2
        run: python pokeprices_scraper_v6.py --sets-file batches/batch2.txt

  batch3:
    runs-on: ubuntu-latest
    timeout-minutes: 300
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: pip install requests beautifulsoup4
      - name: Run batch 3
        run: python pokeprices_scraper_v6.py --sets-file batches/batch3.txt

  batch4:
    runs-on: ubuntu-latest
    timeout-minutes: 300
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: pip install requests beautifulsoup4
      - name: Run batch 4
        run: python pokeprices_scraper_v6.py --sets-file batches/batch4.txt

  # ── Refresh card_trends after all batches complete ────────────────────────
  refresh-trends:
    runs-on: ubuntu-latest
    needs: [batch1, batch2, batch3, batch4]
    timeout-minutes: 30
    steps:
      - name: Refresh card_trends table
        run: |
          # Step 1: Find the actual reference dates in the data
          echo "Finding reference dates..."

          DATES_RESPONSE=$(curl -s -X POST \
            "${SUPABASE_URL}/rest/v1/rpc/get_trend_dates" \
            -H "apikey: ${SUPABASE_SERVICE_ROLE_KEY}" \
            -H "Authorization: Bearer ${SUPABASE_SERVICE_ROLE_KEY}" \
            -H "Content-Type: application/json" \
            -d '{}')

          echo "Dates response: ${DATES_RESPONSE}"

          # If the RPC function doesn't exist, use a raw SQL approach via REST
          # We'll use the simpler approach: call the refresh via a purpose-built function

          # Step 2: Truncate and repopulate card_trends
          # This calls a lightweight Supabase function that does the refresh in chunks
          echo "Refreshing card_trends..."

          REFRESH_RESPONSE=$(curl -s -w "\n%{http_code}" -X POST \
            "${SUPABASE_URL}/rest/v1/rpc/refresh_card_trends_v2" \
            -H "apikey: ${SUPABASE_SERVICE_ROLE_KEY}" \
            -H "Authorization: Bearer ${SUPABASE_SERVICE_ROLE_KEY}" \
            -H "Content-Type: application/json" \
            -d '{}')

          HTTP_CODE=$(echo "$REFRESH_RESPONSE" | tail -1)
          BODY=$(echo "$REFRESH_RESPONSE" | head -n -1)

          if [ "$HTTP_CODE" -ge 200 ] && [ "$HTTP_CODE" -lt 300 ]; then
            echo "card_trends refreshed successfully"
          else
            echo "WARNING: card_trends refresh returned HTTP ${HTTP_CODE}"
            echo "Response: ${BODY}"
            echo ""
            echo "Falling back to Python-based refresh..."
            # Fall back to Python script that does it in chunks
            pip install requests
            python3 - <<'PYTHON_EOF'
          import os, requests, json
          from datetime import date, timedelta

          SUPABASE_URL = os.environ["SUPABASE_URL"]
          SUPABASE_KEY = os.environ["SUPABASE_SERVICE_ROLE_KEY"]
          HEADERS = {
              "apikey": SUPABASE_KEY,
              "Authorization": f"Bearer {SUPABASE_KEY}",
              "Content-Type": "application/json",
              "Prefer": "return=minimal",
          }

          today = date.today().isoformat()

          # Find reference dates by querying daily_prices
          def find_date(days_ago):
              """Find the most recent date in daily_prices on or before N days ago."""
              target = (date.today() - timedelta(days=days_ago)).isoformat()
              r = requests.get(
                  f"{SUPABASE_URL}/rest/v1/daily_prices"
                  f"?select=date&date=lte.{target}&order=date.desc&limit=1",
                  headers=HEADERS,
              )
              data = r.json()
              return data[0]["date"] if data else None

          d7 = find_date(7)
          d30 = find_date(30)
          d90 = find_date(90)
          d180 = find_date(180)
          d365 = find_date(365)
          d2y = find_date(730)
          d5y = find_date(1825)

          print(f"Reference dates: today={today} d7={d7} d30={d30} d90={d90} d180={d180} d365={d365} d2y={d2y} d5y={d5y}")

          # Truncate card_trends
          requests.delete(
              f"{SUPABASE_URL}/rest/v1/card_trends?card_slug=neq.",
              headers=HEADERS,
          )
          print("Truncated card_trends")

          # Build and execute the INSERT via raw SQL (using Supabase's RPC or pg_net)
          # Since we can't run raw SQL via REST, we use a helper RPC function
          # If that doesn't exist, we'll build the data in Python

          # Get all cards with today's prices
          print("Fetching today's prices...")
          offset = 0
          all_rows = []
          while True:
              r = requests.get(
                  f"{SUPABASE_URL}/rest/v1/daily_prices"
                  f"?date=eq.{today}&raw_usd=gt.500&select=card_slug,raw_usd,psa10_usd,psa9_usd"
                  f"&order=card_slug&offset={offset}&limit=1000",
                  headers=HEADERS,
              )
              batch = r.json()
              if not batch:
                  break
              all_rows.extend(batch)
              offset += 1000
              if len(batch) < 1000:
                  break

          print(f"Found {len(all_rows)} cards with prices > $5 today")

          def get_historical_prices(ref_date):
              """Fetch all prices for a reference date, return as dict keyed by card_slug."""
              if not ref_date:
                  return {}
              prices = {}
              offset = 0
              while True:
                  r = requests.get(
                      f"{SUPABASE_URL}/rest/v1/daily_prices"
                      f"?date=eq.{ref_date}&select=card_slug,raw_usd,psa10_usd"
                      f"&order=card_slug&offset={offset}&limit=1000",
                      headers=HEADERS,
                  )
                  batch = r.json()
                  if not batch:
                      break
                  for row in batch:
                      prices[row["card_slug"]] = row
                  offset += 1000
                  if len(batch) < 1000:
                      break
              return prices

          # Fetch historical data for all reference dates
          hist = {}
          for label, ref_date in [("d7", d7), ("d30", d30), ("d90", d90), ("d180", d180), ("d365", d365), ("d2y", d2y), ("d5y", d5y)]:
              print(f"Fetching {label} ({ref_date})...")
              hist[label] = get_historical_prices(ref_date)

          def pct(current, old):
              if not old or old == 0 or not current:
                  return None
              return round(((current - old) / old) * 100, 1)

          # Get card names from cards table
          print("Fetching card metadata...")
          cards_meta = {}
          offset = 0
          while True:
              r = requests.get(
                  f"{SUPABASE_URL}/rest/v1/cards"
                  f"?select=card_slug,card_name,set_name&order=card_slug&offset={offset}&limit=1000",
                  headers=HEADERS,
              )
              batch = r.json()
              if not batch:
                  break
              for row in batch:
                  cards_meta[f"pc-{row['card_slug']}"] = row
              offset += 1000
              if len(batch) < 1000:
                  break

          # Build trend rows
          trend_rows = []
          for row in all_rows:
              slug = row["card_slug"]
              meta = cards_meta.get(slug, {})
              if not meta:
                  continue  # No card metadata found

              def h(period, field="raw_usd"):
                  return hist.get(period, {}).get(slug, {}).get(field)

              trend_row = {
                  "card_slug": meta.get("card_slug", slug.replace("pc-", "")),
                  "card_name": meta.get("card_name"),
                  "set_name": meta.get("set_name"),
                  "current_raw": row.get("raw_usd"),
                  "current_psa10": row.get("psa10_usd"),
                  "current_psa9": row.get("psa9_usd"),
                  "raw_7d_ago": h("d7"),
                  "raw_30d_ago": h("d30"),
                  "raw_90d_ago": h("d90"),
                  "raw_180d_ago": h("d180"),
                  "raw_365d_ago": h("d365"),
                  "raw_2y_ago": h("d2y"),
                  "raw_5y_ago": h("d5y"),
                  "psa10_30d_ago": h("d30", "psa10_usd"),
                  "psa10_90d_ago": h("d90", "psa10_usd"),
                  "raw_pct_7d": pct(row.get("raw_usd"), h("d7")),
                  "raw_pct_30d": pct(row.get("raw_usd"), h("d30")),
                  "raw_pct_90d": pct(row.get("raw_usd"), h("d90")),
                  "raw_pct_180d": pct(row.get("raw_usd"), h("d180")),
                  "raw_pct_365d": pct(row.get("raw_usd"), h("d365")),
                  "raw_pct_2y": pct(row.get("raw_usd"), h("d2y")),
                  "raw_pct_5y": pct(row.get("raw_usd"), h("d5y")),
                  "psa10_pct_30d": pct(row.get("psa10_usd"), h("d30", "psa10_usd")),
                  "psa10_pct_90d": pct(row.get("psa10_usd"), h("d90", "psa10_usd")),
                  "as_of": today,
              }
              trend_rows.append(trend_row)

          # Push in batches
          print(f"Pushing {len(trend_rows)} trend rows...")
          for i in range(0, len(trend_rows), 500):
              batch = trend_rows[i:i+500]
              r = requests.post(
                  f"{SUPABASE_URL}/rest/v1/card_trends",
                  json=batch,
                  headers={**HEADERS, "Prefer": "resolution=merge-duplicates"},
              )
              if r.status_code in (200, 201):
                  print(f"  Pushed {i+len(batch)}/{len(trend_rows)}")
              else:
                  print(f"  ERROR at batch {i}: {r.status_code} {r.text[:200]}")

          print("card_trends refresh complete!")
          PYTHON_EOF
          fi
