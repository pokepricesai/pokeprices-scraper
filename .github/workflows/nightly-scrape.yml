name: Nightly Scrape

on:
  schedule:
    - cron: '0 3 * * *'
  workflow_dispatch:

env:
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}

jobs:
  # —— 6 parallel batch scrape jobs ——————————————————————————————————
  batch1:
    runs-on: ubuntu-latest
    timeout-minutes: 300
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: pip install requests beautifulsoup4
      - name: Run batch 1
        run: python pokeprices_scraper_v7.py --sets-file batches/batch1.txt

  batch2:
    runs-on: ubuntu-latest
    timeout-minutes: 300
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: pip install requests beautifulsoup4
      - name: Run batch 2
        run: python pokeprices_scraper_v7.py --sets-file batches/batch2.txt

  batch3:
    runs-on: ubuntu-latest
    timeout-minutes: 300
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: pip install requests beautifulsoup4
      - name: Run batch 3
        run: python pokeprices_scraper_v7.py --sets-file batches/batch3.txt

  batch4:
    runs-on: ubuntu-latest
    timeout-minutes: 300
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: pip install requests beautifulsoup4
      - name: Run batch 4
        run: python pokeprices_scraper_v7.py --sets-file batches/batch4.txt

  batch5:
    runs-on: ubuntu-latest
    timeout-minutes: 300
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: pip install requests beautifulsoup4
      - name: Run batch 5
        run: python pokeprices_scraper_v7.py --sets-file batches/batch5.txt

  batch6:
    runs-on: ubuntu-latest
    timeout-minutes: 300
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: pip install requests beautifulsoup4
      - name: Run batch 6
        run: python pokeprices_scraper_v7.py --sets-file batches/batch6.txt

  # —— eBay API scraper (runs in parallel with batches) ———————————————
  ebay-scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: pip install requests
      - name: Run eBay scraper
        env:
          EBAY_APP_ID: ${{ secrets.EBAY_APP_ID }}
          EBAY_CERT_ID: ${{ secrets.EBAY_CERT_ID }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: python ebay_scraper.py

  # —— Detect deals (runs after scrapers + analytics) ————————————————
  detect-deals:
    runs-on: ubuntu-latest
    needs: [refresh-and-analytics]
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: pip install requests
      - name: Detect deals
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: python detect_deals.py

  # —— Refresh card_trends + compute analytics ———————————————————————
  refresh-and-analytics:
    runs-on: ubuntu-latest
    needs: [batch1, batch2, batch3, batch4, batch5, batch6, ebay-scrape]
    timeout-minutes: 120
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: pip install requests

      - name: Refresh card_trends
        run: |
          python3 << 'PYEOF'
          import os, requests, json
          from datetime import date, timedelta

          SUPABASE_URL = os.environ["SUPABASE_URL"]
          SUPABASE_KEY = os.environ["SUPABASE_KEY"]
          HEADERS = {
              "apikey": SUPABASE_KEY,
              "Authorization": f"Bearer {SUPABASE_KEY}",
              "Content-Type": "application/json",
          }
          POST_HEADERS = {**HEADERS, "Prefer": "return=minimal"}

          today = date.today()
          today_str = today.isoformat()

          def find_date(days_ago):
              target = (today - timedelta(days=days_ago)).isoformat()
              r = requests.get(
                  f"{SUPABASE_URL}/rest/v1/daily_prices"
                  f"?select=date&date=lte.{target}&order=date.desc&limit=1",
                  headers=HEADERS, timeout=15,
              )
              if r.status_code == 200:
                  data = r.json()
                  if isinstance(data, list) and len(data) > 0:
                      return data[0]["date"]
              return None

          d_today = find_date(0)
          if not d_today:
              d_today = find_date(1)
          if not d_today:
              print("ERROR: No recent price data found!")
              exit(1)

          d7 = find_date(7)
          d30 = find_date(30)
          d90 = find_date(90)
          d180 = find_date(180)
          d365 = find_date(365)
          d2y = find_date(730)
          d5y = find_date(1825)

          print(f"Reference dates: today={d_today} d7={d7} d30={d30} d90={d90} d180={d180} d365={d365} d2y={d2y} d5y={d5y}")

          def fetch_all(endpoint):
              rows = []
              offset = 0
              while True:
                  sep = "&" if "?" in endpoint else "?"
                  r = requests.get(
                      f"{SUPABASE_URL}/rest/v1/{endpoint}{sep}offset={offset}&limit=1000",
                      headers=HEADERS, timeout=30,
                  )
                  if r.status_code != 200:
                      print(f"  WARN: fetch {endpoint} offset={offset}: {r.status_code}")
                      break
                  batch = r.json()
                  if not isinstance(batch, list) or not batch:
                      break
                  rows.extend(batch)
                  offset += 1000
                  if len(batch) < 1000:
                      break
              return rows

          print("Fetching today's prices...")
          today_prices = fetch_all(
              f"daily_prices?date=eq.{d_today}&raw_usd=gt.500"
              f"&select=card_slug,raw_usd,psa10_usd,psa9_usd"
          )
          print(f"  Found {len(today_prices)} cards with prices > $5")

          if not today_prices:
              print("ERROR: No prices found for today!")
              exit(1)

          def get_prices_for_date(ref_date):
              if not ref_date:
                  return {}
              rows = fetch_all(
                  f"daily_prices?date=eq.{ref_date}&select=card_slug,raw_usd,psa10_usd"
              )
              return {row["card_slug"]: row for row in rows}

          hist = {}
          for label, ref_date in [("d7", d7), ("d30", d30), ("d90", d90), ("d180", d180), ("d365", d365), ("d2y", d2y), ("d5y", d5y)]:
              print(f"  Fetching {label} ({ref_date})...")
              hist[label] = get_prices_for_date(ref_date)

          print("Fetching card metadata...")
          cards_meta = {}
          for row in fetch_all("cards?select=card_slug,card_name,set_name"):
              cards_meta[f"pc-{row['card_slug']}"] = row

          def pct(current, old):
              if not old or old == 0 or not current:
                  return None
              return round(((current - old) / old) * 100, 1)

          trend_rows = []
          for row in today_prices:
              slug = row["card_slug"]
              meta = cards_meta.get(slug)
              if not meta:
                  continue

              def h(period, field="raw_usd"):
                  entry = hist.get(period, {}).get(slug)
                  if entry is None:
                      return None
                  return entry.get(field)

              trend_rows.append({
                  "card_slug": meta["card_slug"],
                  "card_name": meta.get("card_name"),
                  "set_name": meta.get("set_name"),
                  "current_raw": row.get("raw_usd"),
                  "current_psa10": row.get("psa10_usd"),
                  "current_psa9": row.get("psa9_usd"),
                  "raw_7d_ago": h("d7"),
                  "raw_30d_ago": h("d30"),
                  "raw_90d_ago": h("d90"),
                  "raw_180d_ago": h("d180"),
                  "raw_365d_ago": h("d365"),
                  "raw_2y_ago": h("d2y"),
                  "raw_5y_ago": h("d5y"),
                  "psa10_30d_ago": h("d30", "psa10_usd"),
                  "psa10_90d_ago": h("d90", "psa10_usd"),
                  "raw_pct_7d": pct(row.get("raw_usd"), h("d7")),
                  "raw_pct_30d": pct(row.get("raw_usd"), h("d30")),
                  "raw_pct_90d": pct(row.get("raw_usd"), h("d90")),
                  "raw_pct_180d": pct(row.get("raw_usd"), h("d180")),
                  "raw_pct_365d": pct(row.get("raw_usd"), h("d365")),
                  "raw_pct_2y": pct(row.get("raw_usd"), h("d2y")),
                  "raw_pct_5y": pct(row.get("raw_usd"), h("d5y")),
                  "psa10_pct_30d": pct(row.get("psa10_usd"), h("d30", "psa10_usd")),
                  "psa10_pct_90d": pct(row.get("psa10_usd"), h("d90", "psa10_usd")),
                  "as_of": d_today,
              })

          print(f"Truncating card_trends...")
          requests.delete(
              f"{SUPABASE_URL}/rest/v1/card_trends?card_slug=neq.",
              headers=POST_HEADERS, timeout=30,
          )

          print(f"Pushing {len(trend_rows)} trend rows...")
          push_headers = {**HEADERS, "Prefer": "resolution=merge-duplicates"}
          pushed = 0
          for i in range(0, len(trend_rows), 500):
              batch = trend_rows[i:i+500]
              r = requests.post(
                  f"{SUPABASE_URL}/rest/v1/card_trends",
                  json=batch, headers=push_headers, timeout=30,
              )
              if r.status_code in (200, 201):
                  pushed += len(batch)
              else:
                  print(f"  ERROR at {i}: {r.status_code} {r.text[:200]}")
          print(f"  Pushed {pushed}/{len(trend_rows)} rows")
          print("card_trends refresh complete!")
          PYEOF

      - name: Scrape set prices (median + value history)
        run: python scrape_set_prices.py

      - name: Compute analytics (metrics, spread, set metrics)
        run: python compute_analytics.py

      - name: Strip Pokemon prefix from set names
        run: |
          curl -s -X POST \
            "$SUPABASE_URL/rest/v1/rpc/strip_pokemon_prefix" \
            -H "apikey: $SUPABASE_KEY" \
            -H "Authorization: Bearer $SUPABASE_KEY" \
            -H "Content-Type: application/json" \
            -d '{}'
          echo "Pokemon prefix stripped from set names"
